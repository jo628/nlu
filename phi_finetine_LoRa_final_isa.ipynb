{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c914f0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c914f0d",
        "outputId": "6365033f-55f4-4a7b-d322-a4437389d297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, evaluate, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.5 datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 fsspec-2025.3.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets peft evaluate scikit-learn numpy pandas matplotlib seaborn bitsandbytes tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c9adc277",
      "metadata": {
        "id": "c9adc277"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel\n",
        ")\n",
        "import evaluate\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8b9b3889",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b9b3889",
        "outputId": "c2ad4f20-df0e-4c1a-b5a1-adda25ea9c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "40634b7a",
      "metadata": {
        "id": "40634b7a"
      },
      "outputs": [],
      "source": [
        "model_name = \"microsoft/phi-2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1e7d9137",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7d9137",
        "outputId": "fc9d8af8-70b5-4930-9b2b-6a639c2d7bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 500 examples from ./invoices_500.json\n",
            "\n",
            "Sample invoice data:\n",
            "Input (truncated):\n",
            "11117 Campbell Brooks Apt. 246\n",
            "\n",
            "Elizabethside, AK 00799\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Description Quantity Unit price\n",
            "redefine cross-media\n",
            "systems 7.65 82.78\n",
            "exploit bleeding-edge\n",
            "action-items 7.0 59.16\n",
            "incubate real-time ROI 4.18 58.32\n",
            "optimize viral\n",
            "deliverables 1.12 18.83\n",
            "\n",
            "Amount excluding tax 140.57\n",
            "\n",
            "Taxes 21.71...\n",
            "\n",
            "Expected Output (sample):\n",
            "{\n",
            "  \"buyer\": {\n",
            "    \"address\": \"65302 Booker Trafficway Apt. 529 Christophermouth, WY 67659\"\n",
            "  },\n",
            "  \"invoice\": {\n",
            "    \"bc_no\": \"lo11165\",\n",
            "    \"date\": \"05.04.1994\",\n",
            "    \"maturity_date\": \"30.04.2008\",\n",
            "    \"number\": 158485\n",
            "  },\n",
            "  \"products\": [\n",
            "    {\n",
            "      \"amount\": 327.14,\n",
            "      \"description\": \"redefine cross-media systems\",\n",
            "      \"quantity\": 7.65,\n",
            "      \"unit_price\": 82.78,\n",
            "      \"vat_amount\": 11.3\n",
            "    },\n",
            "    {\n",
            "      \"amount\": 227.61,\n",
            "      \"description\": \"exploit bleeding-edge action-items\",\n",
            "      ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_file_path = './invoices_500.json'\n",
        "\n",
        "try:\n",
        "    with open(data_file_path, 'r') as f:\n",
        "        invoice_data = json.load(f)\n",
        "    print(f\"Successfully loaded {len(invoice_data)} examples from {data_file_path}\")\n",
        "\n",
        "    print(\"\\nSample invoice data:\")\n",
        "    sample_idx = 0\n",
        "    print(f\"Input (truncated):\\n{invoice_data[sample_idx]['input'][:300]}...\\n\")\n",
        "    print(f\"Expected Output (sample):\\n{json.dumps(invoice_data[sample_idx]['output'], indent=2)[:500]}...\\n\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {data_file_path} was not found.\")\n",
        "    print(\"Please ensure the dataset file exists in the correct location.\")\n",
        "    raise\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error: The file {data_file_path} is not a valid JSON file.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b6ab0a53",
      "metadata": {
        "id": "b6ab0a53"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "    processed_data = []\n",
        "\n",
        "    for item in data:\n",
        "        instruction = \"Extract the structured information from this invoice and format it as JSON:\"\n",
        "        input_text = item[\"input\"]\n",
        "\n",
        "        output_text = json.dumps(item[\"output\"], indent=2)\n",
        "\n",
        "        prompt = f\"{instruction}\\n\\n{input_text}\"\n",
        "\n",
        "        processed_data.append({\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": input_text,\n",
        "            \"output\": output_text,\n",
        "            \"prompt\": prompt\n",
        "        })\n",
        "\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "35cfaf04",
      "metadata": {
        "id": "35cfaf04"
      },
      "outputs": [],
      "source": [
        "processed_data = preprocess_data(invoice_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "01a54a63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01a54a63",
        "outputId": "99118a67-8426-40df-abc7-a1f584afc6da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples: 400, Validation examples: 100\n"
          ]
        }
      ],
      "source": [
        "train_data, val_data = train_test_split(processed_data, test_size=0.2, random_state=42)\n",
        "print(f\"Training examples: {len(train_data)}, Validation examples: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cb81d9c3",
      "metadata": {
        "id": "cb81d9c3"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_dict({\n",
        "    \"instruction\": [item[\"instruction\"] for item in train_data],\n",
        "    \"input\": [item[\"input\"] for item in train_data],\n",
        "    \"output\": [item[\"output\"] for item in train_data],\n",
        "    \"prompt\": [item[\"prompt\"] for item in train_data]\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    \"instruction\": [item[\"instruction\"] for item in val_data],\n",
        "    \"input\": [item[\"input\"] for item in val_data],\n",
        "    \"output\": [item[\"output\"] for item in val_data],\n",
        "    \"prompt\": [item[\"prompt\"] for item in val_data]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "184e348f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "184e348f",
        "outputId": "c5063c2b-97ca-4c8e-aef7-997c751bbab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "37cbdf2b",
      "metadata": {
        "id": "37cbdf2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "1768dff69f284710a34024ef4d720af5",
            "a5c6ecf6e5354518b5793cfa12d89731",
            "3ac8cf8ebc524d9aaecfe288a83485b8",
            "561de6d21bcd47358714ee44891d6e2e",
            "f226b6365c534ccb99f2f6f318c4b1b9",
            "9378a3ed1a664710a746e5b8aac88648",
            "3157184a5be544ab8d598ec470b8436e",
            "c5fd12dd08084ae2bab8f773c5def0d0",
            "277deab1d9e647749141b85fe3fdfe08",
            "097d1de4710f40018b93bc0f7bcec60b",
            "8fb2916307d94f338139a8415995d3c2",
            "524bd1a421ce48899ee0db333ed7779f",
            "756d86736dfd420393559f2b66ab20b6",
            "4009d8bab28946a6bc71f66937293c38",
            "2c6b5864be4845e2bc0ad0b9262629cd",
            "812401f7c2dc410a9652a2bd00eb521c",
            "7dc786acedd04d9c8711e20cbd4fdb9f",
            "f85704225e1144f0916cbd2f97c4b87f",
            "c8550dcfd4054c9290da2cf149d6ae18",
            "6f3e6cc862a84fe4848522e832866746",
            "9655e9f55e184f85acdb0d67ab6b8f2e",
            "d2cb582b2a3147f8bb1b994e0e8e607a"
          ]
        },
        "outputId": "88ff3d17-e512-48a2-9c6d-1a3241613fae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1768dff69f284710a34024ef4d720af5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "524bd1a421ce48899ee0db333ed7779f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization complete\n"
          ]
        }
      ],
      "source": [
        "def tokenization_function(examples):\n",
        "    prompts = []\n",
        "    targets = []\n",
        "\n",
        "    for instruction, input_text, output_text in zip(\n",
        "        examples[\"instruction\"], examples[\"input\"], examples[\"output\"]\n",
        "    ):\n",
        "        prompt = f\"{instruction}\\n\\n{input_text}\\n\\nJSON Result:\\n\"\n",
        "        target = f\"{prompt}{output_text}{tokenizer.eos_token}\"\n",
        "\n",
        "        prompts.append(prompt)\n",
        "        targets.append(target)\n",
        "\n",
        "    tokenized_targets = tokenizer(\n",
        "        targets,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    tokenized_prompts = tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    labels = tokenized_targets[\"input_ids\"].clone()\n",
        "\n",
        "    for i, (prompt_len, target_len) in enumerate(zip(\n",
        "        tokenized_prompts[\"attention_mask\"].sum(dim=1),\n",
        "        tokenized_targets[\"attention_mask\"].sum(dim=1)\n",
        "    )):\n",
        "        labels[i, :prompt_len] = -100\n",
        "\n",
        "    tokenized_targets[\"labels\"] = labels\n",
        "\n",
        "    return tokenized_targets\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    tokenization_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"instruction\", \"input\", \"output\", \"prompt\"]\n",
        ")\n",
        "\n",
        "tokenized_val_dataset = val_dataset.map(\n",
        "    tokenization_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"instruction\", \"input\", \"output\", \"prompt\"]\n",
        ")\n",
        "\n",
        "print(\"Tokenization complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b87cf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509,
          "referenced_widgets": [
            "88f77d86811d442fb556908eeb3585b8",
            "488b4ad0f0344e1fac0dd2751e3a602f",
            "8c6b03253b9d4217a044f91fd8295e39",
            "6a12ca7768624b2ca47a8942ba2a4194",
            "20570eb7da3741edb367489bc34e323a",
            "66261a956b02431f838b732b898831a5",
            "73f8a69f3c0a4fba8772ac203b68dd44",
            "e34b7321c0ea4068b5abc05126e98fd2",
            "9b9eb088551244f8975434cbe9d787ea",
            "962425c078224194a06f25fea6946659",
            "23ed0734b7624995a9c299fd51929f85"
          ]
        },
        "id": "f5b87cf5",
        "outputId": "c43c60ce-4d96-4b56-f86c-a8a4ed79b237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up model for LoRA fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88f77d86811d442fb556908eeb3585b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-491e7a40f6f6>:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,621,440 || all params: 2,782,305,280 || trainable%: 0.0942\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='71' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 71/250 56:28 < 2:26:29, 0.02 it/s, Epoch 1.40/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.679600</td>\n",
              "      <td>1.582295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.317400</td>\n",
              "      <td>1.332488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.212000</td>\n",
              "      <td>1.193602</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        }
      ],
      "source": [
        "print(\"Setting up model for LoRA fine-tuning...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./phi2-invoice-extraction\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=40,\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=5,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=8,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"./phi2-invoice-lora-adapter\")\n",
        "print(\"Model fine-tuning completed and saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56bd230b",
      "metadata": {
        "id": "56bd230b"
      },
      "outputs": [],
      "source": [
        "def clean_and_parse_json(text):\n",
        "    if \"JSON Result:\" in text:\n",
        "        try:\n",
        "            json_text = text.split(\"JSON Result:\")[1].strip()\n",
        "            return json.loads(json_text)\n",
        "        except json.JSONDecodeError:\n",
        "            try:\n",
        "                json_text = re.sub(r',\\s*}', '}', json_text)\n",
        "                json_text = re.sub(r',\\s*]', ']', json_text)\n",
        "                json_text = re.sub(r'(\\w+)(?=\\s*:)', r'\"\\1\"', json_text)\n",
        "                return json.loads(json_text)\n",
        "            except (json.JSONDecodeError, NameError):\n",
        "                return {}\n",
        "    return {}\n",
        "\n",
        "def evaluate_json_extraction(original, predicted):\n",
        "    if isinstance(original, str):\n",
        "        try:\n",
        "            original = json.loads(original)\n",
        "        except json.JSONDecodeError:\n",
        "            original = {}\n",
        "\n",
        "    if isinstance(predicted, str):\n",
        "        try:\n",
        "            predicted = json.loads(predicted)\n",
        "        except json.JSONDecodeError:\n",
        "            predicted = {}\n",
        "\n",
        "    def flatten_dict(d, parent_key='', sep='.'):\n",
        "        items = []\n",
        "        for k, v in d.items():\n",
        "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "            if isinstance(v, dict):\n",
        "                items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "            elif isinstance(v, list) and all(isinstance(i, dict) for i in v):\n",
        "                for i, item in enumerate(v):\n",
        "                    list_key = f\"{new_key}[{i}]\"\n",
        "                    items.extend(flatten_dict(item, list_key, sep=sep).items())\n",
        "            else:\n",
        "                items.append((new_key, v))\n",
        "        return dict(items)\n",
        "\n",
        "    flat_original = flatten_dict(original)\n",
        "    flat_predicted = flatten_dict(predicted)\n",
        "\n",
        "    all_keys = set(flat_original.keys())\n",
        "    found_keys = set(flat_original.keys()).intersection(set(flat_predicted.keys()))\n",
        "\n",
        "    exact_matches = 0\n",
        "    field_accuracy = {}\n",
        "\n",
        "    for key in all_keys:\n",
        "        if key in flat_predicted and flat_original[key] == flat_predicted[key]:\n",
        "            exact_matches += 1\n",
        "            field_accuracy[key] = 1.0\n",
        "        else:\n",
        "            field_accuracy[key] = 0.0\n",
        "\n",
        "    results = {\n",
        "        \"exact_match\": exact_matches,\n",
        "        \"fields_found\": len(found_keys),\n",
        "        \"total_fields\": len(all_keys),\n",
        "        \"field_accuracy\": field_accuracy,\n",
        "        \"overall_accuracy\": exact_matches / len(all_keys) if all_keys else 0\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "eval_examples = val_dataset.select(range(min(5, len(val_dataset))))\n",
        "\n",
        "def generate_extraction(model, tokenizer, prompt, max_length=2048):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=max_length,\n",
        "            temperature=0.1,\n",
        "            top_p=0.75,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    result_prefix = \"JSON Result:\"\n",
        "    if result_prefix in generated_text:\n",
        "        extraction = generated_text.split(result_prefix)[1].strip()\n",
        "    else:\n",
        "        extraction = generated_text.strip()\n",
        "\n",
        "    try:\n",
        "        json_result = json.loads(extraction)\n",
        "        extraction = json.dumps(json_result, indent=2)\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "\n",
        "    return extraction\n",
        "\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "\n",
        "results = {\n",
        "    \"overall_accuracy\": [],\n",
        "    \"fields_found_pct\": [],\n",
        "    \"exact_match_pct\": [],\n",
        "    \"field_accuracies\": {}\n",
        "}\n",
        "\n",
        "for example in tqdm(eval_examples):\n",
        "    prompt = f\"{example['instruction']}\\n\\n{example['input']}\\n\\nJSON Result:\\n\"\n",
        "\n",
        "    predicted_output = generate_extraction(model, tokenizer, prompt)\n",
        "\n",
        "    try:\n",
        "        predicted_json = json.loads(predicted_output)\n",
        "    except json.JSONDecodeError:\n",
        "        predicted_json = {}\n",
        "\n",
        "    try:\n",
        "        expected_json = json.loads(example[\"output\"])\n",
        "    except json.JSONDecodeError:\n",
        "        expected_json = {}\n",
        "\n",
        "    eval_result = evaluate_json_extraction(expected_json, predicted_json)\n",
        "\n",
        "    results[\"overall_accuracy\"].append(eval_result[\"overall_accuracy\"])\n",
        "    results[\"fields_found_pct\"].append(eval_result[\"fields_found\"] / eval_result[\"total_fields\"])\n",
        "    results[\"exact_match_pct\"].append(eval_result[\"exact_match\"] / eval_result[\"total_fields\"])\n",
        "\n",
        "    for field, accuracy in eval_result[\"field_accuracy\"].items():\n",
        "        if field not in results[\"field_accuracies\"]:\n",
        "            results[\"field_accuracies\"][field] = []\n",
        "        results[\"field_accuracies\"][field].append(accuracy)\n",
        "\n",
        "    print(f\"\\n--- Example Evaluation ---\")\n",
        "    print(f\"Input: {example['input'][:100]}...\")\n",
        "    print(f\"Expected (truncated): {example['output'][:100]}...\")\n",
        "    print(f\"Predicted (truncated): {predicted_output[:100]}...\")\n",
        "    print(f\"Accuracy: {eval_result['overall_accuracy']:.2f}\")\n",
        "\n",
        "avg_accuracy = np.mean(results[\"overall_accuracy\"])\n",
        "avg_fields_found = np.mean(results[\"fields_found_pct\"])\n",
        "avg_exact_match = np.mean(results[\"exact_match_pct\"])\n",
        "\n",
        "print(f\"\\n--- Overall Evaluation Results ---\")\n",
        "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "print(f\"Average Fields Found: {avg_fields_found:.4f}\")\n",
        "print(f\"Average Exact Match: {avg_exact_match:.4f}\")\n",
        "\n",
        "category_accuracy = {}\n",
        "for field in results[\"field_accuracies\"]:\n",
        "    category = field.split('.')[0] if '.' in field else field.split('[')[0] if '[' in field else field\n",
        "    if category not in category_accuracy:\n",
        "        category_accuracy[category] = []\n",
        "    category_accuracy[category].extend(results[\"field_accuracies\"][field])\n",
        "\n",
        "print(\"\\n--- Category-specific Accuracy ---\")\n",
        "for category, accuracies in category_accuracy.items():\n",
        "    print(f\"{category}: {np.mean(accuracies):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb2ff96",
      "metadata": {
        "id": "dfb2ff96"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "metrics = ['Overall Accuracy', 'Fields Found', 'Exact Match']\n",
        "values = [avg_accuracy, avg_fields_found, avg_exact_match]\n",
        "plt.bar(metrics, values, color=['blue', 'green', 'orange'])\n",
        "plt.title('Invoice Extraction Performance Metrics')\n",
        "plt.ylabel('Score (0-1)')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "for i, v in enumerate(values):\n",
        "    plt.text(i, v + 0.05, f'{v:.2f}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('overall_metrics.png')\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "categories = list(category_accuracy.keys())\n",
        "category_values = [np.mean(category_accuracy[cat]) for cat in categories]\n",
        "\n",
        "sorted_indices = np.argsort(category_values)\n",
        "sorted_categories = [categories[i] for i in sorted_indices]\n",
        "sorted_values = [category_values[i] for i in sorted_indices]\n",
        "\n",
        "plt.barh(sorted_categories, sorted_values, color='skyblue')\n",
        "plt.title('Accuracy by Information Category')\n",
        "plt.xlabel('Accuracy (0-1)')\n",
        "plt.xlim(0, 1)\n",
        "\n",
        "for i, v in enumerate(sorted_values):\n",
        "    plt.text(v + 0.05, i, f'{v:.2f}', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('category_accuracy.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50ee607",
      "metadata": {
        "id": "c50ee607"
      },
      "outputs": [],
      "source": [
        "def demo_invoice_extraction(model, tokenizer, input_text):\n",
        "    instruction = \"Extract the structured information from this invoice and format it as JSON:\"\n",
        "    prompt = f\"{instruction}\\n\\n{input_text}\\n\\nJSON Result:\\n\"\n",
        "\n",
        "    extraction = generate_extraction(model, tokenizer, prompt)\n",
        "\n",
        "    try:\n",
        "        structured_data = json.loads(extraction)\n",
        "    except json.JSONDecodeError:\n",
        "        structured_data = {\"error\": \"Could not parse generated output as valid JSON\"}\n",
        "\n",
        "    return {\n",
        "        \"input\": input_text,\n",
        "        \"extracted_text\": extraction,\n",
        "        \"structured_data\": structured_data\n",
        "    }\n",
        "\n",
        "test_invoice = \"\"\"\n",
        "TechSolutions Inc.\n",
        "123 Innovation Drive\n",
        "Silicon Valley, CA 94025\n",
        "\n",
        "INVOICE #INV-2025-051\n",
        "Date: April 15, 2025\n",
        "Due Date: May 15, 2025\n",
        "\n",
        "Bill To:\n",
        "John Smith\n",
        "123 Client Street\n",
        "Clientville, CA 90210\n",
        "Email: john.smith@example.com\n",
        "\n",
        "Item                           Quantity    Price       Amount\n",
        "Cloud Storage: Premium tier    1           $99.99      $99.99\n",
        "Technical Support: 24/7        1           $199.99     $199.99\n",
        "Software License: Enterprise   5           $299.99     $1,499.95\n",
        "\n",
        "Subtotal:                                             $1,799.93\n",
        "Tax (8.5%):                                           $152.99\n",
        "Total Due:                                            $1,952.92\n",
        "\n",
        "Payment Terms: Net 30\n",
        "Payment Method: Credit Card\n",
        "\n",
        "Thank you for your business!\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Demo: Structured Output ---\")\n",
        "result = demo_invoice_extraction(model, tokenizer, test_invoice)\n",
        "\n",
        "print(\"Input Invoice (truncated):\")\n",
        "print(test_invoice[:200] + \"...\")\n",
        "print(\"\\nExtracted JSON:\")\n",
        "print(result[\"extracted_text\"])\n",
        "print(\"\\nStructured Data (parsed JSON):\")\n",
        "print(json.dumps(result[\"structured_data\"], indent=2))\n",
        "\n",
        "print(\"\\n--- Demo: Batch Processing ---\")\n",
        "test_invoices = [invoice_data[0][\"input\"], invoice_data[1][\"input\"], test_invoice]\n",
        "batch_results = []\n",
        "\n",
        "for invoice in test_invoices:\n",
        "    extraction = demo_invoice_extraction(model, tokenizer, invoice)\n",
        "    batch_results.append(extraction[\"structured_data\"])\n",
        "\n",
        "def flatten_for_dataframe(nested_dict, prefix=''):\n",
        "    flat_dict = {}\n",
        "    for key, value in nested_dict.items():\n",
        "        if isinstance(value, dict):\n",
        "            flat_dict.update(flatten_for_dataframe(value, f\"{prefix}{key}_\"))\n",
        "        elif isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):\n",
        "            flat_dict.update(flatten_for_dataframe(value[0], f\"{prefix}{key}_item0_\"))\n",
        "        else:\n",
        "            flat_dict[f\"{prefix}{key}\"] = value\n",
        "    return flat_dict\n",
        "\n",
        "flat_batch_results = [flatten_for_dataframe(result) for result in batch_results]\n",
        "batch_df = pd.DataFrame(flat_batch_results)\n",
        "\n",
        "print(\"Batch Results (sample columns):\")\n",
        "display_columns = batch_df.columns[:5] if len(batch_df.columns) > 5 else batch_df.columns\n",
        "print(batch_df[display_columns])\n",
        "\n",
        "print(\"\\n--- Project Summary ---\")\n",
        "print(\"1. Successfully fine-tuned Phi-2 model for structured JSON invoice extraction using LoRA\")\n",
        "print(f\"2. Achieved {avg_accuracy:.2f} overall accuracy on the validation set\")\n",
        "print(f\"3. Model can extract complex nested JSON with multiple categories of information\")\n",
        "print(f\"4. Top-performing categories: {sorted(category_accuracy.items(), key=lambda x: np.mean(x[1]), reverse=True)[0][0]}\")\n",
        "print(\"5. Provides output as structured JSON for seamless integration with systems\")\n",
        "print(\"6. LoRA fine-tuning allowed efficient adaptation of Phi-2 with minimal parameters\")\n",
        "\n",
        "print(\"\\nFine-tuned model can be used in production settings for automated invoice processing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4350e3ad",
      "metadata": {
        "id": "4350e3ad"
      },
      "source": [
        "## Saving Fine-tuned Phi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f11b9fb",
      "metadata": {
        "id": "5f11b9fb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "model_save_dir = \"./phi2-invoice-extractor-final\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "print(\"Saving the model and related files...\")\n",
        "\n",
        "model.save_pretrained(f\"{model_save_dir}/lora_adapter\")\n",
        "\n",
        "tokenizer.save_pretrained(f\"{model_save_dir}/tokenizer\")\n",
        "\n",
        "target_modules = list(peft_config.target_modules) if hasattr(peft_config.target_modules, '__iter__') else peft_config.target_modules\n",
        "\n",
        "config_info = {\n",
        "    \"base_model\": model_name,\n",
        "    \"author\": \"Zeyad-Diaa-1242\",\n",
        "    \"date_created\": \"2025-05-11 19:05:13\",\n",
        "    \"task\": \"Structured JSON Invoice Information Extraction\",\n",
        "    \"training_examples\": len(train_data),\n",
        "    \"validation_examples\": len(val_data),\n",
        "    \"accuracy\": float(avg_accuracy),\n",
        "    \"fields_extracted\": list(category_accuracy.keys()),\n",
        "    \"lora_config\": {\n",
        "        \"r\": int(peft_config.r),\n",
        "        \"lora_alpha\": float(peft_config.lora_alpha),\n",
        "        \"lora_dropout\": float(peft_config.lora_dropout),\n",
        "        \"target_modules\": target_modules\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f\"{model_save_dir}/model_info.json\", \"w\") as f:\n",
        "    json.dump(config_info, f, indent=2)\n",
        "\n",
        "# Save a README with usage instructions without f-strings\n",
        "with open(f\"{model_save_dir}/README.md\", \"w\") as f:\n",
        "    f.write(\"# Phi-2 Structured JSON Invoice Extractor\\n\\n\")\n",
        "    f.write(\"## Model Information\\n\")\n",
        "    f.write(f\"- Base Model: {model_name}\\n\")\n",
        "    f.write(\"- Task: Structured JSON Invoice Information Extraction\\n\")\n",
        "    f.write(f\"- Author: {config_info['author']}\\n\")\n",
        "    f.write(f\"- Date Created: {config_info['date_created']}\\n\")\n",
        "    f.write(f\"- Accuracy: {avg_accuracy:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"## Usage Instructions\\n\\n\")\n",
        "    f.write(\"### Loading the Model\\n\\n\")\n",
        "    f.write(\"```python\\n\")\n",
        "    f.write(\"from transformers import AutoModelForCausalLM, AutoTokenizer\\n\")\n",
        "    f.write(\"from peft import PeftModel\\n\\n\")\n",
        "    f.write(\"# Load the base model\\n\")\n",
        "    f.write(f\"base_model = AutoModelForCausalLM.from_pretrained(\\n\")\n",
        "    f.write(f'    \"{model_name}\",\\n')\n",
        "    f.write(\"    torch_dtype=torch.float16,\\n\")\n",
        "    f.write(\"    device_map=\\\"auto\\\",\\n\")\n",
        "    f.write(\"    trust_remote_code=True\\n\")\n",
        "    f.write(\")\\n\\n\")\n",
        "    f.write(\"# Load the tokenizer\\n\")\n",
        "    f.write(\"tokenizer = AutoTokenizer.from_pretrained(\\\"./tokenizer\\\")\\n\\n\")\n",
        "    f.write(\"# Load the LoRA adapter\\n\")\n",
        "    f.write(\"model = PeftModel.from_pretrained(base_model, \\\"./lora_adapter\\\")\\n\")\n",
        "    f.write(\"```\\n\\n\")\n",
        "\n",
        "    f.write(\"### Making Predictions\\n\\n\")\n",
        "    f.write(\"```python\\n\")\n",
        "    f.write(\"def extract_invoice_info(invoice_text):\\n\")\n",
        "    f.write(\"    # Format the prompt\\n\")\n",
        "    f.write('    prompt = f\"Extract the structured information from this invoice and format it as JSON:\\\\n\\\\n{invoice_text}\\\\n\\\\nJSON Result:\\\\n\"\\n\\n')\n",
        "    f.write(\"    # Generate output\\n\")\n",
        "    f.write(\"    inputs = tokenizer(prompt, return_tensors=\\\"pt\\\").to(model.device)\\n\")\n",
        "    f.write(\"    with torch.no_grad():\\n\")\n",
        "    f.write(\"        outputs = model.generate(\\n\")\n",
        "    f.write(\"            input_ids=inputs[\\\"input_ids\\\"],\\n\")\n",
        "    f.write(\"            attention_mask=inputs[\\\"attention_mask\\\"],\\n\")\n",
        "    f.write(\"            max_length=2048,\\n\")\n",
        "    f.write(\"            temperature=0.1,\\n\")\n",
        "    f.write(\"            top_p=0.75,\\n\")\n",
        "    f.write(\"            do_sample=False\\n\")\n",
        "    f.write(\"        )\\n\\n\")\n",
        "    f.write(\"    # Decode the output\\n\")\n",
        "    f.write(\"    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\n\")\n",
        "    f.write(\"    # Extract just the result part\\n\")\n",
        "    f.write('    if \"JSON Result:\" in generated_text:\\n')\n",
        "    f.write('        extraction = generated_text.split(\"JSON Result:\")[1].strip()\\n')\n",
        "    f.write(\"    else:\\n\")\n",
        "    f.write(\"        extraction = generated_text.strip()\\n\")\n",
        "    f.write(\"        \\n\")\n",
        "    f.write(\"    # Parse as JSON\\n\")\n",
        "    f.write(\"    try:\\n\")\n",
        "    f.write(\"        result = json.loads(extraction)\\n\")\n",
        "    f.write(\"        return result\\n\")\n",
        "    f.write(\"    except json.JSONDecodeError:\\n\")\n",
        "    f.write(\"        return {\\\"error\\\": \\\"Failed to parse output as JSON\\\"}\\n\")\n",
        "    f.write(\"```\\n\\n\")\n",
        "\n",
        "    f.write(\"## Extracted Fields\\n\")\n",
        "    f.write(\"The model can extract the following information categories from invoices:\\n\")\n",
        "    f.write(f\"{', '.join(list(category_accuracy.keys()))}\\n\\n\")\n",
        "\n",
        "    f.write(\"## Performance\\n\")\n",
        "    f.write(f\"- Overall Accuracy: {avg_accuracy:.4f}\\n\")\n",
        "    f.write(f\"- Fields Found: {avg_fields_found:.4f}\\n\")\n",
        "    f.write(f\"- Exact Match: {avg_exact_match:.4f}\\n\")\n",
        "\n",
        "# Create a sample inference script without f-strings\n",
        "with open(f\"{model_save_dir}/inference.py\", \"w\") as f:\n",
        "    f.write(\"import torch\\n\")\n",
        "    f.write(\"import json\\n\")\n",
        "    f.write(\"from transformers import AutoModelForCausalLM, AutoTokenizer\\n\")\n",
        "    f.write(\"from peft import PeftModel\\n\")\n",
        "    f.write(\"import argparse\\n\\n\")\n",
        "\n",
        "    f.write(\"def load_model(base_model_name, adapter_path, tokenizer_path):\\n\")\n",
        "    f.write('    print(f\"Loading base model: {base_model_name}\")\\n')\n",
        "    f.write(\"    base_model = AutoModelForCausalLM.from_pretrained(\\n\")\n",
        "    f.write(\"        base_model_name,\\n\")\n",
        "    f.write(\"        torch_dtype=torch.float16,\\n\")\n",
        "    f.write('        device_map=\"auto\",\\n')\n",
        "    f.write(\"        trust_remote_code=True\\n\")\n",
        "    f.write(\"    )\\n\\n\")\n",
        "\n",
        "    f.write('    print(f\"Loading tokenizer from: {tokenizer_path}\")\\n')\n",
        "    f.write(\"    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\\n\")\n",
        "    f.write(\"    tokenizer.pad_token = tokenizer.eos_token\\n\\n\")\n",
        "\n",
        "    f.write('    print(f\"Loading LoRA adapter from: {adapter_path}\")\\n')\n",
        "    f.write(\"    model = PeftModel.from_pretrained(base_model, adapter_path)\\n\\n\")\n",
        "\n",
        "    f.write(\"    return model, tokenizer\\n\\n\")\n",
        "\n",
        "    f.write(\"def extract_invoice_info(model, tokenizer, invoice_text):\\n\")\n",
        "    f.write(\"    # Format the prompt\\n\")\n",
        "    f.write('    prompt = f\"Extract the structured information from this invoice and format it as JSON:\\\\n\\\\n{invoice_text}\\\\n\\\\nJSON Result:\\\\n\"\\n\\n')\n",
        "\n",
        "    f.write(\"    # Generate output\\n\")\n",
        "    f.write('    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\\n')\n",
        "    f.write(\"    with torch.no_grad():\\n\")\n",
        "    f.write(\"        outputs = model.generate(\\n\")\n",
        "    f.write('            input_ids=inputs[\"input_ids\"],\\n')\n",
        "    f.write('            attention_mask=inputs[\"attention_mask\"],\\n')\n",
        "    f.write(\"            max_length=2048,\\n\")\n",
        "    f.write(\"            temperature=0.1,\\n\")\n",
        "    f.write(\"            top_p=0.75,\\n\")\n",
        "    f.write(\"            do_sample=False\\n\")\n",
        "    f.write(\"        )\\n\\n\")\n",
        "\n",
        "    f.write(\"    # Decode the output\\n\")\n",
        "    f.write(\"    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\n\")\n",
        "\n",
        "    f.write(\"    # Extract just the result part\\n\")\n",
        "    f.write('    if \"JSON Result:\" in generated_text:\\n')\n",
        "    f.write('        extraction = generated_text.split(\"JSON Result:\")[1].strip()\\n')\n",
        "    f.write(\"    else:\\n\")\n",
        "    f.write(\"        extraction = generated_text.strip()\\n\")\n",
        "    f.write(\"        \\n\")\n",
        "    f.write(\"    return extraction\\n\\n\")\n",
        "\n",
        "    f.write(\"def main():\\n\")\n",
        "    f.write('    parser = argparse.ArgumentParser(description=\"Extract structured JSON information from invoices using fine-tuned Phi-2 model\")\\n')\n",
        "    f.write('    parser.add_argument(\"--invoice\", type=str, required=True, help=\"Path to the invoice text file\")\\n')\n",
        "    f.write('    parser.add_argument(\"--base_model\", type=str, default=\"microsoft/phi-2\", help=\"Base model name or path\")\\n')\n",
        "    f.write('    parser.add_argument(\"--adapter\", type=str, default=\"./lora_adapter\", help=\"Path to the LoRA adapter\")\\n')\n",
        "    f.write('    parser.add_argument(\"--tokenizer\", type=str, default=\"./tokenizer\", help=\"Path to the tokenizer\")\\n')\n",
        "    f.write('    parser.add_argument(\"--output\", type=str, default=\"extraction_result.json\", help=\"Output JSON file path\")\\n\\n')\n",
        "\n",
        "    f.write(\"    args = parser.parse_args()\\n\\n\")\n",
        "\n",
        "    f.write(\"    # Load the model\\n\")\n",
        "    f.write(\"    model, tokenizer = load_model(args.base_model, args.adapter, args.tokenizer)\\n\\n\")\n",
        "\n",
        "    f.write(\"    # Read the invoice text\\n\")\n",
        "    f.write(\"    with open(args.invoice, 'r') as f:\\n\")\n",
        "    f.write(\"        invoice_text = f.read()\\n\\n\")\n",
        "\n",
        "    f.write(\"    # Extract information\\n\")\n",
        "    f.write('    print(\"Extracting structured information from invoice...\")\\n')\n",
        "    f.write(\"    extracted_text = extract_invoice_info(model, tokenizer, invoice_text)\\n\\n\")\n",
        "\n",
        "    f.write(\"    # Try to parse as JSON\\n\")\n",
        "    f.write(\"    try:\\n\")\n",
        "    f.write(\"        structured_data = json.loads(extracted_text)\\n\")\n",
        "    f.write(\"    except json.JSONDecodeError:\\n\")\n",
        "    f.write('        print(\"Warning: Could not parse output as valid JSON\")\\n')\n",
        "    f.write('        structured_data = {\"error\": \"Invalid JSON output\", \"raw_text\": extracted_text}\\n\\n')\n",
        "\n",
        "    f.write(\"    # Save results\\n\")\n",
        "    f.write(\"    results = {\\n\")\n",
        "    f.write('        \"extracted_text\": extracted_text,\\n')\n",
        "    f.write('        \"structured_data\": structured_data\\n')\n",
        "    f.write(\"    }\\n\\n\")\n",
        "\n",
        "    f.write(\"    with open(args.output, 'w') as f:\\n\")\n",
        "    f.write(\"        json.dump(results, f, indent=2)\\n\\n\")\n",
        "\n",
        "    f.write('    print(f\"Results saved to {args.output}\")\\n')\n",
        "    f.write('    print(\"\\\\nExtracted Structure:\")\\n')\n",
        "    f.write(\"    print(json.dumps(structured_data, indent=2))\\n\\n\")\n",
        "\n",
        "    f.write('if __name__ == \"__main__\":\\n')\n",
        "    f.write(\"    main()\\n\")\n",
        "\n",
        "# Create a zip file of the model directory\n",
        "print(\"Creating zip archive of model files...\")\n",
        "zip_path = \"./phi2-invoice-json-extractor.zip\"\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(model_save_dir):\n",
        "        for file in files:\n",
        "            zipf.write(os.path.join(root, file),\n",
        "                       os.path.relpath(os.path.join(root, file),\n",
        "                                       os.path.join(model_save_dir, '..')))\n",
        "\n",
        "print(f\"Model saved successfully to {zip_path}\")\n",
        "print(\"Downloading model zip file...\")\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_path)\n",
        "\n",
        "print(\"Model download initiated.\")\n",
        "print(\"Save this file and use it to deploy your structured JSON invoice extraction model!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1768dff69f284710a34024ef4d720af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5c6ecf6e5354518b5793cfa12d89731",
              "IPY_MODEL_3ac8cf8ebc524d9aaecfe288a83485b8",
              "IPY_MODEL_561de6d21bcd47358714ee44891d6e2e"
            ],
            "layout": "IPY_MODEL_f226b6365c534ccb99f2f6f318c4b1b9"
          }
        },
        "a5c6ecf6e5354518b5793cfa12d89731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9378a3ed1a664710a746e5b8aac88648",
            "placeholder": "​",
            "style": "IPY_MODEL_3157184a5be544ab8d598ec470b8436e",
            "value": "Map: 100%"
          }
        },
        "3ac8cf8ebc524d9aaecfe288a83485b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5fd12dd08084ae2bab8f773c5def0d0",
            "max": 400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_277deab1d9e647749141b85fe3fdfe08",
            "value": 400
          }
        },
        "561de6d21bcd47358714ee44891d6e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_097d1de4710f40018b93bc0f7bcec60b",
            "placeholder": "​",
            "style": "IPY_MODEL_8fb2916307d94f338139a8415995d3c2",
            "value": " 400/400 [00:02&lt;00:00, 134.55 examples/s]"
          }
        },
        "f226b6365c534ccb99f2f6f318c4b1b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9378a3ed1a664710a746e5b8aac88648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3157184a5be544ab8d598ec470b8436e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5fd12dd08084ae2bab8f773c5def0d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "277deab1d9e647749141b85fe3fdfe08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "097d1de4710f40018b93bc0f7bcec60b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fb2916307d94f338139a8415995d3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "524bd1a421ce48899ee0db333ed7779f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_756d86736dfd420393559f2b66ab20b6",
              "IPY_MODEL_4009d8bab28946a6bc71f66937293c38",
              "IPY_MODEL_2c6b5864be4845e2bc0ad0b9262629cd"
            ],
            "layout": "IPY_MODEL_812401f7c2dc410a9652a2bd00eb521c"
          }
        },
        "756d86736dfd420393559f2b66ab20b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dc786acedd04d9c8711e20cbd4fdb9f",
            "placeholder": "​",
            "style": "IPY_MODEL_f85704225e1144f0916cbd2f97c4b87f",
            "value": "Map: 100%"
          }
        },
        "4009d8bab28946a6bc71f66937293c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8550dcfd4054c9290da2cf149d6ae18",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f3e6cc862a84fe4848522e832866746",
            "value": 100
          }
        },
        "2c6b5864be4845e2bc0ad0b9262629cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9655e9f55e184f85acdb0d67ab6b8f2e",
            "placeholder": "​",
            "style": "IPY_MODEL_d2cb582b2a3147f8bb1b994e0e8e607a",
            "value": " 100/100 [00:00&lt;00:00, 254.90 examples/s]"
          }
        },
        "812401f7c2dc410a9652a2bd00eb521c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc786acedd04d9c8711e20cbd4fdb9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f85704225e1144f0916cbd2f97c4b87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8550dcfd4054c9290da2cf149d6ae18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3e6cc862a84fe4848522e832866746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9655e9f55e184f85acdb0d67ab6b8f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2cb582b2a3147f8bb1b994e0e8e607a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88f77d86811d442fb556908eeb3585b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_488b4ad0f0344e1fac0dd2751e3a602f",
              "IPY_MODEL_8c6b03253b9d4217a044f91fd8295e39",
              "IPY_MODEL_6a12ca7768624b2ca47a8942ba2a4194"
            ],
            "layout": "IPY_MODEL_20570eb7da3741edb367489bc34e323a"
          }
        },
        "488b4ad0f0344e1fac0dd2751e3a602f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66261a956b02431f838b732b898831a5",
            "placeholder": "​",
            "style": "IPY_MODEL_73f8a69f3c0a4fba8772ac203b68dd44",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8c6b03253b9d4217a044f91fd8295e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e34b7321c0ea4068b5abc05126e98fd2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b9eb088551244f8975434cbe9d787ea",
            "value": 2
          }
        },
        "6a12ca7768624b2ca47a8942ba2a4194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_962425c078224194a06f25fea6946659",
            "placeholder": "​",
            "style": "IPY_MODEL_23ed0734b7624995a9c299fd51929f85",
            "value": " 2/2 [00:22&lt;00:00,  9.59s/it]"
          }
        },
        "20570eb7da3741edb367489bc34e323a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66261a956b02431f838b732b898831a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f8a69f3c0a4fba8772ac203b68dd44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e34b7321c0ea4068b5abc05126e98fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b9eb088551244f8975434cbe9d787ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "962425c078224194a06f25fea6946659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ed0734b7624995a9c299fd51929f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}